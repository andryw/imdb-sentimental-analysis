{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/andryw/Projects/tutorials/embeddings/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data = pd.read_csv(\"data/unlabeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "train_data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "punkt_tokenizer = nltk.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.archive.org/details/lovefromastranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/loosechangeguide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0ksqelmgn8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "reviews = list(unlabel_data[\"review\"]) + list(train_data[\"review\"])\n",
    "\n",
    "sentences = []\n",
    "for r in reviews:\n",
    "    sentences += utils.text_to_sentences(r, punkt_tokenizer)\n",
    "    \n",
    "sentences_word_list = [utils.normalize_text(sentence, remove_stop_words=False).split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 16:02:00,951 : INFO : collecting all words and their counts\n",
      "2019-01-24 16:02:00,956 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-24 16:02:01,013 : INFO : PROGRESS: at sentence #10000, processed 228386 words, keeping 17381 word types\n",
      "2019-01-24 16:02:01,059 : INFO : PROGRESS: at sentence #20000, processed 450696 words, keeping 24739 word types\n",
      "2019-01-24 16:02:01,089 : INFO : PROGRESS: at sentence #30000, processed 676534 words, keeping 29983 word types\n",
      "2019-01-24 16:02:01,138 : INFO : PROGRESS: at sentence #40000, processed 901155 words, keeping 34165 word types\n",
      "2019-01-24 16:02:01,168 : INFO : PROGRESS: at sentence #50000, processed 1122654 words, keeping 37737 word types\n",
      "2019-01-24 16:02:01,209 : INFO : PROGRESS: at sentence #60000, processed 1349724 words, keeping 41004 word types\n",
      "2019-01-24 16:02:01,258 : INFO : PROGRESS: at sentence #70000, processed 1576974 words, keeping 43944 word types\n",
      "2019-01-24 16:02:01,290 : INFO : PROGRESS: at sentence #80000, processed 1802672 words, keeping 46461 word types\n",
      "2019-01-24 16:02:01,335 : INFO : PROGRESS: at sentence #90000, processed 2020732 words, keeping 48717 word types\n",
      "2019-01-24 16:02:01,372 : INFO : PROGRESS: at sentence #100000, processed 2248354 words, keeping 50917 word types\n",
      "2019-01-24 16:02:01,418 : INFO : PROGRESS: at sentence #110000, processed 2474025 words, keeping 53078 word types\n",
      "2019-01-24 16:02:01,457 : INFO : PROGRESS: at sentence #120000, processed 2700945 words, keeping 55211 word types\n",
      "2019-01-24 16:02:01,499 : INFO : PROGRESS: at sentence #130000, processed 2927805 words, keeping 57005 word types\n",
      "2019-01-24 16:02:01,540 : INFO : PROGRESS: at sentence #140000, processed 3147960 words, keeping 58752 word types\n",
      "2019-01-24 16:02:01,589 : INFO : PROGRESS: at sentence #150000, processed 3370770 words, keeping 60459 word types\n",
      "2019-01-24 16:02:01,623 : INFO : PROGRESS: at sentence #160000, processed 3593433 words, keeping 62116 word types\n",
      "2019-01-24 16:02:01,667 : INFO : PROGRESS: at sentence #170000, processed 3822790 words, keeping 63728 word types\n",
      "2019-01-24 16:02:01,709 : INFO : PROGRESS: at sentence #180000, processed 4046892 words, keeping 65310 word types\n",
      "2019-01-24 16:02:01,756 : INFO : PROGRESS: at sentence #190000, processed 4275844 words, keeping 66903 word types\n",
      "2019-01-24 16:02:01,796 : INFO : PROGRESS: at sentence #200000, processed 4503021 words, keeping 68310 word types\n",
      "2019-01-24 16:02:01,839 : INFO : PROGRESS: at sentence #210000, processed 4733100 words, keeping 69728 word types\n",
      "2019-01-24 16:02:01,880 : INFO : PROGRESS: at sentence #220000, processed 4956134 words, keeping 71166 word types\n",
      "2019-01-24 16:02:01,922 : INFO : PROGRESS: at sentence #230000, processed 5177498 words, keeping 72414 word types\n",
      "2019-01-24 16:02:01,964 : INFO : PROGRESS: at sentence #240000, processed 5400376 words, keeping 73719 word types\n",
      "2019-01-24 16:02:02,005 : INFO : PROGRESS: at sentence #250000, processed 5626220 words, keeping 75007 word types\n",
      "2019-01-24 16:02:02,049 : INFO : PROGRESS: at sentence #260000, processed 5852165 words, keeping 76193 word types\n",
      "2019-01-24 16:02:02,081 : INFO : PROGRESS: at sentence #270000, processed 6075511 words, keeping 77378 word types\n",
      "2019-01-24 16:02:02,125 : INFO : PROGRESS: at sentence #280000, processed 6294196 words, keeping 78550 word types\n",
      "2019-01-24 16:02:02,163 : INFO : PROGRESS: at sentence #290000, processed 6521195 words, keeping 79729 word types\n",
      "2019-01-24 16:02:02,208 : INFO : PROGRESS: at sentence #300000, processed 6746461 words, keeping 80790 word types\n",
      "2019-01-24 16:02:02,257 : INFO : PROGRESS: at sentence #310000, processed 6973912 words, keeping 81916 word types\n",
      "2019-01-24 16:02:02,301 : INFO : PROGRESS: at sentence #320000, processed 7195139 words, keeping 82986 word types\n",
      "2019-01-24 16:02:02,350 : INFO : PROGRESS: at sentence #330000, processed 7419900 words, keeping 84035 word types\n",
      "2019-01-24 16:02:02,390 : INFO : PROGRESS: at sentence #340000, processed 7639712 words, keeping 85133 word types\n",
      "2019-01-24 16:02:02,433 : INFO : PROGRESS: at sentence #350000, processed 7866172 words, keeping 86155 word types\n",
      "2019-01-24 16:02:02,474 : INFO : PROGRESS: at sentence #360000, processed 8088731 words, keeping 87120 word types\n",
      "2019-01-24 16:02:02,520 : INFO : PROGRESS: at sentence #370000, processed 8312401 words, keeping 88114 word types\n",
      "2019-01-24 16:02:02,561 : INFO : PROGRESS: at sentence #380000, processed 8535369 words, keeping 89159 word types\n",
      "2019-01-24 16:02:02,612 : INFO : PROGRESS: at sentence #390000, processed 8758806 words, keeping 90137 word types\n",
      "2019-01-24 16:02:02,672 : INFO : PROGRESS: at sentence #400000, processed 8983629 words, keeping 91042 word types\n",
      "2019-01-24 16:02:02,748 : INFO : PROGRESS: at sentence #410000, processed 9210426 words, keeping 91990 word types\n",
      "2019-01-24 16:02:02,815 : INFO : PROGRESS: at sentence #420000, processed 9432101 words, keeping 92852 word types\n",
      "2019-01-24 16:02:02,863 : INFO : PROGRESS: at sentence #430000, processed 9656524 words, keeping 93811 word types\n",
      "2019-01-24 16:02:02,908 : INFO : PROGRESS: at sentence #440000, processed 9883379 words, keeping 94762 word types\n",
      "2019-01-24 16:02:02,975 : INFO : PROGRESS: at sentence #450000, processed 10111109 words, keeping 95599 word types\n",
      "2019-01-24 16:02:03,045 : INFO : PROGRESS: at sentence #460000, processed 10333018 words, keeping 96461 word types\n",
      "2019-01-24 16:02:03,112 : INFO : PROGRESS: at sentence #470000, processed 10554440 words, keeping 97302 word types\n",
      "2019-01-24 16:02:03,164 : INFO : PROGRESS: at sentence #480000, processed 10777044 words, keeping 98115 word types\n",
      "2019-01-24 16:02:03,208 : INFO : PROGRESS: at sentence #490000, processed 10996991 words, keeping 98942 word types\n",
      "2019-01-24 16:02:03,249 : INFO : PROGRESS: at sentence #500000, processed 11218460 words, keeping 99781 word types\n",
      "2019-01-24 16:02:03,312 : INFO : PROGRESS: at sentence #510000, processed 11447595 words, keeping 100696 word types\n",
      "2019-01-24 16:02:03,382 : INFO : PROGRESS: at sentence #520000, processed 11675398 words, keeping 101497 word types\n",
      "2019-01-24 16:02:03,448 : INFO : PROGRESS: at sentence #530000, processed 11899492 words, keeping 102397 word types\n",
      "2019-01-24 16:02:03,488 : INFO : PROGRESS: at sentence #540000, processed 12125950 words, keeping 103426 word types\n",
      "2019-01-24 16:02:03,533 : INFO : PROGRESS: at sentence #550000, processed 12351093 words, keeping 104515 word types\n",
      "2019-01-24 16:02:03,575 : INFO : PROGRESS: at sentence #560000, processed 12570786 words, keeping 105566 word types\n",
      "2019-01-24 16:02:03,619 : INFO : PROGRESS: at sentence #570000, processed 12799190 words, keeping 106574 word types\n",
      "2019-01-24 16:02:03,665 : INFO : PROGRESS: at sentence #580000, processed 13017707 words, keeping 107496 word types\n",
      "2019-01-24 16:02:03,707 : INFO : PROGRESS: at sentence #590000, processed 13237717 words, keeping 108364 word types\n",
      "2019-01-24 16:02:03,751 : INFO : PROGRESS: at sentence #600000, processed 13461679 words, keeping 109234 word types\n",
      "2019-01-24 16:02:03,794 : INFO : PROGRESS: at sentence #610000, processed 13681861 words, keeping 110026 word types\n",
      "2019-01-24 16:02:03,834 : INFO : PROGRESS: at sentence #620000, processed 13904671 words, keeping 110922 word types\n",
      "2019-01-24 16:02:03,894 : INFO : PROGRESS: at sentence #630000, processed 14126361 words, keeping 111684 word types\n",
      "2019-01-24 16:02:03,976 : INFO : PROGRESS: at sentence #640000, processed 14346468 words, keeping 112449 word types\n",
      "2019-01-24 16:02:04,060 : INFO : PROGRESS: at sentence #650000, processed 14569961 words, keeping 113312 word types\n",
      "2019-01-24 16:02:04,130 : INFO : PROGRESS: at sentence #660000, processed 14787209 words, keeping 114009 word types\n",
      "2019-01-24 16:02:04,165 : INFO : PROGRESS: at sentence #670000, processed 15008817 words, keeping 114732 word types\n",
      "2019-01-24 16:02:04,211 : INFO : PROGRESS: at sentence #680000, processed 15232353 words, keeping 115532 word types\n",
      "2019-01-24 16:02:04,269 : INFO : PROGRESS: at sentence #690000, processed 15455463 words, keeping 116274 word types\n",
      "2019-01-24 16:02:04,340 : INFO : PROGRESS: at sentence #700000, processed 15677325 words, keeping 116936 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 16:02:04,404 : INFO : PROGRESS: at sentence #710000, processed 15898729 words, keeping 117642 word types\n",
      "2019-01-24 16:02:04,464 : INFO : PROGRESS: at sentence #720000, processed 16124893 words, keeping 118307 word types\n",
      "2019-01-24 16:02:04,520 : INFO : PROGRESS: at sentence #730000, processed 16348485 words, keeping 118951 word types\n",
      "2019-01-24 16:02:04,555 : INFO : PROGRESS: at sentence #740000, processed 16572057 words, keeping 119708 word types\n",
      "2019-01-24 16:02:04,607 : INFO : PROGRESS: at sentence #750000, processed 16796401 words, keeping 120414 word types\n",
      "2019-01-24 16:02:04,682 : INFO : PROGRESS: at sentence #760000, processed 17018535 words, keeping 121088 word types\n",
      "2019-01-24 16:02:04,763 : INFO : PROGRESS: at sentence #770000, processed 17243121 words, keeping 121744 word types\n",
      "2019-01-24 16:02:04,821 : INFO : PROGRESS: at sentence #780000, processed 17456218 words, keeping 122445 word types\n",
      "2019-01-24 16:02:04,862 : INFO : PROGRESS: at sentence #790000, processed 17678724 words, keeping 123132 word types\n",
      "2019-01-24 16:02:04,885 : INFO : collected 123505 word types from a corpus of 17798270 raw words and 795538 sentences\n",
      "2019-01-24 16:02:04,886 : INFO : Loading a fresh vocabulary\n",
      "2019-01-24 16:02:04,990 : INFO : effective_min_count=2 retains 74452 unique words (60% of original 123505, drops 49053)\n",
      "2019-01-24 16:02:04,990 : INFO : effective_min_count=2 leaves 17749217 word corpus (99% of original 17798270, drops 49053)\n",
      "2019-01-24 16:02:05,151 : INFO : deleting the raw counts dictionary of 123505 items\n",
      "2019-01-24 16:02:05,154 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-01-24 16:02:05,155 : INFO : downsampling leaves estimated 13317951 word corpus (75.0% of prior 17749217)\n",
      "2019-01-24 16:02:05,363 : INFO : estimated required memory for 74452 words and 300 dimensions: 215910800 bytes\n",
      "2019-01-24 16:02:05,363 : INFO : resetting layer weights\n",
      "2019-01-24 16:02:06,158 : INFO : training model with 3 workers on 74452 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-01-24 16:02:07,167 : INFO : EPOCH 1 - PROGRESS: at 5.35% examples, 712960 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:08,179 : INFO : EPOCH 1 - PROGRESS: at 10.48% examples, 695721 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:09,181 : INFO : EPOCH 1 - PROGRESS: at 14.99% examples, 665099 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:10,208 : INFO : EPOCH 1 - PROGRESS: at 18.87% examples, 623880 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:11,236 : INFO : EPOCH 1 - PROGRESS: at 23.81% examples, 628381 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-24 16:02:12,254 : INFO : EPOCH 1 - PROGRESS: at 29.38% examples, 646226 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:13,264 : INFO : EPOCH 1 - PROGRESS: at 34.59% examples, 652094 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:14,275 : INFO : EPOCH 1 - PROGRESS: at 40.84% examples, 674026 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:15,290 : INFO : EPOCH 1 - PROGRESS: at 47.08% examples, 689903 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:16,301 : INFO : EPOCH 1 - PROGRESS: at 53.35% examples, 703591 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:17,301 : INFO : EPOCH 1 - PROGRESS: at 58.47% examples, 702139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:18,302 : INFO : EPOCH 1 - PROGRESS: at 64.78% examples, 713167 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:19,306 : INFO : EPOCH 1 - PROGRESS: at 69.97% examples, 711570 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:20,318 : INFO : EPOCH 1 - PROGRESS: at 76.30% examples, 719745 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:21,321 : INFO : EPOCH 1 - PROGRESS: at 82.56% examples, 726901 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:22,327 : INFO : EPOCH 1 - PROGRESS: at 88.92% examples, 733318 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:23,334 : INFO : EPOCH 1 - PROGRESS: at 95.32% examples, 739855 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:24,334 : INFO : EPOCH 1 - PROGRESS: at 99.88% examples, 731983 words/s, in_qsize 2, out_qsize 1\n",
      "2019-01-24 16:02:24,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-24 16:02:24,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 16:02:24,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 16:02:24,354 : INFO : EPOCH - 1 : training on 17798270 raw words (13316733 effective words) took 18.2s, 731987 effective words/s\n",
      "2019-01-24 16:02:25,368 : INFO : EPOCH 2 - PROGRESS: at 3.51% examples, 466260 words/s, in_qsize 4, out_qsize 1\n",
      "2019-01-24 16:02:26,376 : INFO : EPOCH 2 - PROGRESS: at 8.91% examples, 592199 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:27,379 : INFO : EPOCH 2 - PROGRESS: at 12.66% examples, 561188 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:02:28,395 : INFO : EPOCH 2 - PROGRESS: at 18.42% examples, 610594 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:29,413 : INFO : EPOCH 2 - PROGRESS: at 24.51% examples, 650163 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:30,419 : INFO : EPOCH 2 - PROGRESS: at 30.50% examples, 674332 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:31,422 : INFO : EPOCH 2 - PROGRESS: at 36.43% examples, 690621 words/s, in_qsize 5, out_qsize 1\n",
      "2019-01-24 16:02:32,428 : INFO : EPOCH 2 - PROGRESS: at 39.94% examples, 662958 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:02:33,436 : INFO : EPOCH 2 - PROGRESS: at 43.87% examples, 647024 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:34,439 : INFO : EPOCH 2 - PROGRESS: at 47.71% examples, 632951 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:35,449 : INFO : EPOCH 2 - PROGRESS: at 53.64% examples, 646666 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:36,450 : INFO : EPOCH 2 - PROGRESS: at 59.61% examples, 659233 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:02:37,461 : INFO : EPOCH 2 - PROGRESS: at 62.74% examples, 639714 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:38,466 : INFO : EPOCH 2 - PROGRESS: at 67.39% examples, 638546 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:39,470 : INFO : EPOCH 2 - PROGRESS: at 73.46% examples, 649531 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:40,478 : INFO : EPOCH 2 - PROGRESS: at 79.58% examples, 658966 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:41,488 : INFO : EPOCH 2 - PROGRESS: at 85.83% examples, 668023 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:02:42,499 : INFO : EPOCH 2 - PROGRESS: at 91.55% examples, 672806 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:43,517 : INFO : EPOCH 2 - PROGRESS: at 97.30% examples, 676803 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:43,960 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-24 16:02:43,963 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 16:02:43,979 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 16:02:43,980 : INFO : EPOCH - 2 : training on 17798270 raw words (13316776 effective words) took 19.6s, 678695 effective words/s\n",
      "2019-01-24 16:02:44,996 : INFO : EPOCH 3 - PROGRESS: at 5.57% examples, 739449 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:46,006 : INFO : EPOCH 3 - PROGRESS: at 11.23% examples, 743165 words/s, in_qsize 4, out_qsize 1\n",
      "2019-01-24 16:02:47,010 : INFO : EPOCH 3 - PROGRESS: at 16.90% examples, 748457 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:48,012 : INFO : EPOCH 3 - PROGRESS: at 21.60% examples, 717841 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:49,017 : INFO : EPOCH 3 - PROGRESS: at 27.19% examples, 724381 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:50,034 : INFO : EPOCH 3 - PROGRESS: at 32.61% examples, 722480 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:51,042 : INFO : EPOCH 3 - PROGRESS: at 38.37% examples, 728356 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:52,047 : INFO : EPOCH 3 - PROGRESS: at 43.60% examples, 723792 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:02:53,053 : INFO : EPOCH 3 - PROGRESS: at 48.94% examples, 721794 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:54,066 : INFO : EPOCH 3 - PROGRESS: at 54.46% examples, 722565 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:55,068 : INFO : EPOCH 3 - PROGRESS: at 60.06% examples, 724618 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 16:02:56,072 : INFO : EPOCH 3 - PROGRESS: at 65.60% examples, 725552 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:57,095 : INFO : EPOCH 3 - PROGRESS: at 71.41% examples, 728258 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:58,096 : INFO : EPOCH 3 - PROGRESS: at 76.81% examples, 726847 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:02:59,124 : INFO : EPOCH 3 - PROGRESS: at 82.45% examples, 726926 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:00,127 : INFO : EPOCH 3 - PROGRESS: at 86.38% examples, 713594 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:01,130 : INFO : EPOCH 3 - PROGRESS: at 92.00% examples, 715397 words/s, in_qsize 4, out_qsize 1\n",
      "2019-01-24 16:03:02,137 : INFO : EPOCH 3 - PROGRESS: at 97.76% examples, 717672 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:03:02,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-24 16:03:02,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 16:03:02,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 16:03:02,519 : INFO : EPOCH - 3 : training on 17798270 raw words (13317483 effective words) took 18.5s, 718561 effective words/s\n",
      "2019-01-24 16:03:03,533 : INFO : EPOCH 4 - PROGRESS: at 5.57% examples, 738902 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:04,538 : INFO : EPOCH 4 - PROGRESS: at 11.06% examples, 733849 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:05,540 : INFO : EPOCH 4 - PROGRESS: at 16.38% examples, 727723 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:06,560 : INFO : EPOCH 4 - PROGRESS: at 22.21% examples, 736111 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:07,561 : INFO : EPOCH 4 - PROGRESS: at 27.79% examples, 739612 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:08,569 : INFO : EPOCH 4 - PROGRESS: at 33.40% examples, 739976 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:03:09,574 : INFO : EPOCH 4 - PROGRESS: at 38.37% examples, 728807 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:03:10,576 : INFO : EPOCH 4 - PROGRESS: at 43.76% examples, 727258 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:11,578 : INFO : EPOCH 4 - PROGRESS: at 49.44% examples, 730176 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:12,587 : INFO : EPOCH 4 - PROGRESS: at 55.18% examples, 733370 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:13,590 : INFO : EPOCH 4 - PROGRESS: at 60.93% examples, 735735 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:14,599 : INFO : EPOCH 4 - PROGRESS: at 66.61% examples, 737326 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:15,601 : INFO : EPOCH 4 - PROGRESS: at 72.26% examples, 738582 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:16,603 : INFO : EPOCH 4 - PROGRESS: at 77.93% examples, 739051 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:17,604 : INFO : EPOCH 4 - PROGRESS: at 83.73% examples, 740561 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:18,604 : INFO : EPOCH 4 - PROGRESS: at 89.48% examples, 741847 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:19,613 : INFO : EPOCH 4 - PROGRESS: at 95.15% examples, 742185 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:20,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-24 16:03:20,452 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 16:03:20,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 16:03:20,461 : INFO : EPOCH - 4 : training on 17798270 raw words (13318053 effective words) took 17.9s, 742460 effective words/s\n",
      "2019-01-24 16:03:21,468 : INFO : EPOCH 5 - PROGRESS: at 5.70% examples, 760019 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:22,473 : INFO : EPOCH 5 - PROGRESS: at 11.23% examples, 748214 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:23,493 : INFO : EPOCH 5 - PROGRESS: at 16.66% examples, 737756 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:24,510 : INFO : EPOCH 5 - PROGRESS: at 20.98% examples, 694488 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:25,511 : INFO : EPOCH 5 - PROGRESS: at 26.28% examples, 698982 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:26,511 : INFO : EPOCH 5 - PROGRESS: at 31.12% examples, 689620 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:27,517 : INFO : EPOCH 5 - PROGRESS: at 36.83% examples, 699312 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:28,519 : INFO : EPOCH 5 - PROGRESS: at 41.52% examples, 690228 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:29,546 : INFO : EPOCH 5 - PROGRESS: at 45.39% examples, 668961 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:30,561 : INFO : EPOCH 5 - PROGRESS: at 49.43% examples, 654963 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:31,561 : INFO : EPOCH 5 - PROGRESS: at 53.64% examples, 646344 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:32,578 : INFO : EPOCH 5 - PROGRESS: at 58.15% examples, 642095 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:33,579 : INFO : EPOCH 5 - PROGRESS: at 62.41% examples, 635810 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:34,597 : INFO : EPOCH 5 - PROGRESS: at 66.89% examples, 632743 words/s, in_qsize 4, out_qsize 1\n",
      "2019-01-24 16:03:35,603 : INFO : EPOCH 5 - PROGRESS: at 71.41% examples, 630680 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:36,619 : INFO : EPOCH 5 - PROGRESS: at 75.79% examples, 626633 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:03:37,630 : INFO : EPOCH 5 - PROGRESS: at 80.32% examples, 624583 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:38,638 : INFO : EPOCH 5 - PROGRESS: at 84.80% examples, 622377 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-24 16:03:39,653 : INFO : EPOCH 5 - PROGRESS: at 90.54% examples, 629168 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-24 16:03:40,654 : INFO : EPOCH 5 - PROGRESS: at 95.59% examples, 631244 words/s, in_qsize 4, out_qsize 1\n",
      "2019-01-24 16:03:41,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-24 16:03:41,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 16:03:41,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 16:03:41,612 : INFO : EPOCH - 5 : training on 17798270 raw words (13318649 effective words) took 21.1s, 629835 effective words/s\n",
      "2019-01-24 16:03:41,612 : INFO : training on a 88991350 raw words (66587694 effective words) took 95.5s, 697587 effective words/s\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec(sentences_word_list, \n",
    "                                        workers=3, size=300, min_count=2, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 16:03:54,537 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.4863153100013733),\n",
       " ('princess', 0.4820154309272766),\n",
       " ('prince', 0.466062068939209),\n",
       " ('dorff', 0.4572073519229889),\n",
       " ('countess', 0.4429877698421478),\n",
       " ('jane', 0.42872318625450134),\n",
       " ('belle', 0.42114999890327454),\n",
       " ('bride', 0.41396430134773254),\n",
       " ('kings', 0.41230010986328125),\n",
       " ('mary', 0.4052666127681732)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#utils.normalize_text(sentence) without sample=1e-3 window=5\n",
    "word2vec.wv.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "train_data, test_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['review_cleaned'] = train_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "test_data['review_cleaned'] = test_data.review.apply(lambda x: utils.normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_vectors(text, model):\n",
    "    total_words = 0\n",
    "    final_vector = np.zeros(model.vector_size)\n",
    "    for word in text.split(\" \"):\n",
    "        if word in model.vocab:\n",
    "            final_vector += model.get_vector(word)\n",
    "            total_words+=1\n",
    "    return final_vector / total_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = [get_avg_vectors(x, model.wv) for x in train_data['review_cleaned']]\n",
    "train_vectors=np.array(train_vectors)\n",
    "\n",
    "test_vectors = [get_avg_vectors(x, model.wv) for x in test_data['review_cleaned']]\n",
    "test_vectors=np.array(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_vectors\n",
    "y_train = train_data.sentiment\n",
    "\n",
    "x_test = test_vectors\n",
    "y_test  = test_data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(verbose=True, n_jobs=3)\n",
    "logistic_regression = logistic_regression.fit(x_train, y_train)\n",
    "metrics.roc_auc_score(logistic_regression.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\"n_jobs\": 3, \"silent\": True, \"n_estimators\": 100, 'objective':'binary:logistic'}\n",
    "# xgboost_classifier = xgb.XGBClassifier(**params)\n",
    "\n",
    "# xgboost_classifier.fit(x_train, y_train, verbose=True, eval_metric='auc',\n",
    "#                       eval_set=[(x_train, y_train), (x_test, y_test)],)\n",
    "\n",
    "# metrics.roc_auc_score(xgboost_classifier.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabel_data['review_cleaned'] = unlabel_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "unlabel_vectors = [get_avg_vectors(x, word2vec_model.wv) for x in unlabel_data['review_cleaned']]\n",
    "unlabel_vectors=np.array(unlabel_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_predict(unlabel_data, \n",
    "                   logistic_regression.predict(unlabel_vectors), \n",
    "                   \"logistic_word2vec_avg_window-5_with-stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
