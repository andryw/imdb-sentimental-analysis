{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/andryw/Projects/tutorials/embeddings/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data = pd.read_csv(\"data/unlabeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "train_data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "punkt_tokenizer = nltk.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.archive.org/details/lovefromastranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/loosechangeguide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0ksqelmgn8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "reviews = list(unlabel_data[\"review\"]) + list(train_data[\"review\"])\n",
    "\n",
    "sentences = []\n",
    "for r in reviews:\n",
    "    sentences += utils.text_to_sentences(r, punkt_tokenizer)\n",
    "    \n",
    "sentences_word_list = [utils.normalize_text(sentence, remove_stop_words=False).split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [gensim.models.doc2vec.TaggedDocument(words=words, tags=[str(i)]) \n",
    "               for i, words in enumerate(sentences_word_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-23 16:15:06,266 : INFO : collecting all words and their counts\n",
      "2019-01-23 16:15:06,266 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-01-23 16:15:06,312 : INFO : PROGRESS: at example #10000, processed 228386 words (5010645/s), 17381 word types, 10000 tags\n",
      "2019-01-23 16:15:06,357 : INFO : PROGRESS: at example #20000, processed 450696 words (4984374/s), 24739 word types, 20000 tags\n",
      "2019-01-23 16:15:06,404 : INFO : PROGRESS: at example #30000, processed 676534 words (4864372/s), 29983 word types, 30000 tags\n",
      "2019-01-23 16:15:06,451 : INFO : PROGRESS: at example #40000, processed 901155 words (4859707/s), 34165 word types, 40000 tags\n",
      "2019-01-23 16:15:06,505 : INFO : PROGRESS: at example #50000, processed 1122654 words (4149805/s), 37737 word types, 50000 tags\n",
      "2019-01-23 16:15:06,552 : INFO : PROGRESS: at example #60000, processed 1349724 words (4855041/s), 41004 word types, 60000 tags\n",
      "2019-01-23 16:15:06,603 : INFO : PROGRESS: at example #70000, processed 1576974 words (4527415/s), 43944 word types, 70000 tags\n",
      "2019-01-23 16:15:06,650 : INFO : PROGRESS: at example #80000, processed 1802672 words (4809899/s), 46461 word types, 80000 tags\n",
      "2019-01-23 16:15:06,700 : INFO : PROGRESS: at example #90000, processed 2020732 words (4444953/s), 48717 word types, 90000 tags\n",
      "2019-01-23 16:15:06,754 : INFO : PROGRESS: at example #100000, processed 2248354 words (4273432/s), 50917 word types, 100000 tags\n",
      "2019-01-23 16:15:06,802 : INFO : PROGRESS: at example #110000, processed 2474025 words (4732323/s), 53078 word types, 110000 tags\n",
      "2019-01-23 16:15:06,850 : INFO : PROGRESS: at example #120000, processed 2700945 words (4738934/s), 55211 word types, 120000 tags\n",
      "2019-01-23 16:15:06,898 : INFO : PROGRESS: at example #130000, processed 2927805 words (4765950/s), 57005 word types, 130000 tags\n",
      "2019-01-23 16:15:06,948 : INFO : PROGRESS: at example #140000, processed 3147960 words (4469763/s), 58752 word types, 140000 tags\n",
      "2019-01-23 16:15:07,000 : INFO : PROGRESS: at example #150000, processed 3370770 words (4293205/s), 60459 word types, 150000 tags\n",
      "2019-01-23 16:15:07,049 : INFO : PROGRESS: at example #160000, processed 3593433 words (4662675/s), 62116 word types, 160000 tags\n",
      "2019-01-23 16:15:07,098 : INFO : PROGRESS: at example #170000, processed 3822790 words (4701229/s), 63728 word types, 170000 tags\n",
      "2019-01-23 16:15:07,152 : INFO : PROGRESS: at example #180000, processed 4046892 words (4175216/s), 65310 word types, 180000 tags\n",
      "2019-01-23 16:15:07,201 : INFO : PROGRESS: at example #190000, processed 4275844 words (4699800/s), 66903 word types, 190000 tags\n",
      "2019-01-23 16:15:07,255 : INFO : PROGRESS: at example #200000, processed 4503021 words (4259236/s), 68310 word types, 200000 tags\n",
      "2019-01-23 16:15:07,303 : INFO : PROGRESS: at example #210000, processed 4733100 words (4769782/s), 69728 word types, 210000 tags\n",
      "2019-01-23 16:15:07,351 : INFO : PROGRESS: at example #220000, processed 4956134 words (4690301/s), 71166 word types, 220000 tags\n",
      "2019-01-23 16:15:07,398 : INFO : PROGRESS: at example #230000, processed 5177498 words (4746014/s), 72414 word types, 230000 tags\n",
      "2019-01-23 16:15:07,448 : INFO : PROGRESS: at example #240000, processed 5400376 words (4607302/s), 73719 word types, 240000 tags\n",
      "2019-01-23 16:15:07,496 : INFO : PROGRESS: at example #250000, processed 5626220 words (4755314/s), 75007 word types, 250000 tags\n",
      "2019-01-23 16:15:07,547 : INFO : PROGRESS: at example #260000, processed 5852165 words (4492141/s), 76193 word types, 260000 tags\n",
      "2019-01-23 16:15:07,595 : INFO : PROGRESS: at example #270000, processed 6075511 words (4663064/s), 77378 word types, 270000 tags\n",
      "2019-01-23 16:15:07,646 : INFO : PROGRESS: at example #280000, processed 6294196 words (4336868/s), 78550 word types, 280000 tags\n",
      "2019-01-23 16:15:07,696 : INFO : PROGRESS: at example #290000, processed 6521195 words (4600002/s), 79729 word types, 290000 tags\n",
      "2019-01-23 16:15:07,751 : INFO : PROGRESS: at example #300000, processed 6746461 words (4104034/s), 80790 word types, 300000 tags\n",
      "2019-01-23 16:15:07,801 : INFO : PROGRESS: at example #310000, processed 6973912 words (4616507/s), 81916 word types, 310000 tags\n",
      "2019-01-23 16:15:07,855 : INFO : PROGRESS: at example #320000, processed 7195139 words (4124050/s), 82986 word types, 320000 tags\n",
      "2019-01-23 16:15:07,904 : INFO : PROGRESS: at example #330000, processed 7419900 words (4624749/s), 84035 word types, 330000 tags\n",
      "2019-01-23 16:15:07,953 : INFO : PROGRESS: at example #340000, processed 7639712 words (4576710/s), 85133 word types, 340000 tags\n",
      "2019-01-23 16:15:08,014 : INFO : PROGRESS: at example #350000, processed 7866172 words (3709179/s), 86155 word types, 350000 tags\n",
      "2019-01-23 16:15:08,062 : INFO : PROGRESS: at example #360000, processed 8088731 words (4696842/s), 87120 word types, 360000 tags\n",
      "2019-01-23 16:15:08,113 : INFO : PROGRESS: at example #370000, processed 8312401 words (4410653/s), 88114 word types, 370000 tags\n",
      "2019-01-23 16:15:08,163 : INFO : PROGRESS: at example #380000, processed 8535369 words (4474073/s), 89159 word types, 380000 tags\n",
      "2019-01-23 16:15:08,217 : INFO : PROGRESS: at example #390000, processed 8758806 words (4232605/s), 90137 word types, 390000 tags\n",
      "2019-01-23 16:15:08,267 : INFO : PROGRESS: at example #400000, processed 8983629 words (4502927/s), 91042 word types, 400000 tags\n",
      "2019-01-23 16:15:08,317 : INFO : PROGRESS: at example #410000, processed 9210426 words (4601721/s), 91990 word types, 410000 tags\n",
      "2019-01-23 16:15:08,366 : INFO : PROGRESS: at example #420000, processed 9432101 words (4506478/s), 92852 word types, 420000 tags\n",
      "2019-01-23 16:15:08,416 : INFO : PROGRESS: at example #430000, processed 9656524 words (4556619/s), 93811 word types, 430000 tags\n",
      "2019-01-23 16:15:08,472 : INFO : PROGRESS: at example #440000, processed 9883379 words (4119826/s), 94762 word types, 440000 tags\n",
      "2019-01-23 16:15:08,524 : INFO : PROGRESS: at example #450000, processed 10111109 words (4435144/s), 95599 word types, 450000 tags\n",
      "2019-01-23 16:15:08,573 : INFO : PROGRESS: at example #460000, processed 10333018 words (4537353/s), 96461 word types, 460000 tags\n",
      "2019-01-23 16:15:08,624 : INFO : PROGRESS: at example #470000, processed 10554440 words (4374983/s), 97302 word types, 470000 tags\n",
      "2019-01-23 16:15:08,673 : INFO : PROGRESS: at example #480000, processed 10777044 words (4560128/s), 98115 word types, 480000 tags\n",
      "2019-01-23 16:15:08,728 : INFO : PROGRESS: at example #490000, processed 10996991 words (4019655/s), 98942 word types, 490000 tags\n",
      "2019-01-23 16:15:08,778 : INFO : PROGRESS: at example #500000, processed 11218460 words (4500514/s), 99781 word types, 500000 tags\n",
      "2019-01-23 16:15:08,829 : INFO : PROGRESS: at example #510000, processed 11447595 words (4578219/s), 100696 word types, 510000 tags\n",
      "2019-01-23 16:15:08,879 : INFO : PROGRESS: at example #520000, processed 11675398 words (4589197/s), 101497 word types, 520000 tags\n",
      "2019-01-23 16:15:08,929 : INFO : PROGRESS: at example #530000, processed 11899492 words (4527874/s), 102397 word types, 530000 tags\n",
      "2019-01-23 16:15:08,979 : INFO : PROGRESS: at example #540000, processed 12125950 words (4503341/s), 103426 word types, 540000 tags\n",
      "2019-01-23 16:15:09,030 : INFO : PROGRESS: at example #550000, processed 12351093 words (4509229/s), 104515 word types, 550000 tags\n",
      "2019-01-23 16:15:09,717 : INFO : PROGRESS: at example #560000, processed 12570786 words (319725/s), 105566 word types, 560000 tags\n",
      "2019-01-23 16:15:09,769 : INFO : PROGRESS: at example #570000, processed 12799190 words (4466421/s), 106574 word types, 570000 tags\n",
      "2019-01-23 16:15:09,818 : INFO : PROGRESS: at example #580000, processed 13017707 words (4474197/s), 107496 word types, 580000 tags\n",
      "2019-01-23 16:15:09,868 : INFO : PROGRESS: at example #590000, processed 13237717 words (4473300/s), 108364 word types, 590000 tags\n",
      "2019-01-23 16:15:09,924 : INFO : PROGRESS: at example #600000, processed 13461679 words (4023019/s), 109234 word types, 600000 tags\n",
      "2019-01-23 16:15:09,976 : INFO : PROGRESS: at example #610000, processed 13681861 words (4261947/s), 110026 word types, 610000 tags\n",
      "2019-01-23 16:15:10,031 : INFO : PROGRESS: at example #620000, processed 13904671 words (4088209/s), 110922 word types, 620000 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-23 16:15:10,081 : INFO : PROGRESS: at example #630000, processed 14126361 words (4482618/s), 111684 word types, 630000 tags\n",
      "2019-01-23 16:15:10,131 : INFO : PROGRESS: at example #640000, processed 14346468 words (4460568/s), 112449 word types, 640000 tags\n",
      "2019-01-23 16:15:10,181 : INFO : PROGRESS: at example #650000, processed 14569961 words (4476005/s), 113312 word types, 650000 tags\n",
      "2019-01-23 16:15:10,237 : INFO : PROGRESS: at example #660000, processed 14787209 words (3885529/s), 114009 word types, 660000 tags\n",
      "2019-01-23 16:15:10,288 : INFO : PROGRESS: at example #670000, processed 15008817 words (4448780/s), 114732 word types, 670000 tags\n",
      "2019-01-23 16:15:10,338 : INFO : PROGRESS: at example #680000, processed 15232353 words (4448201/s), 115532 word types, 680000 tags\n",
      "2019-01-23 16:15:10,389 : INFO : PROGRESS: at example #690000, processed 15455463 words (4424023/s), 116274 word types, 690000 tags\n",
      "2019-01-23 16:15:10,467 : INFO : PROGRESS: at example #700000, processed 15677325 words (2861325/s), 116936 word types, 700000 tags\n",
      "2019-01-23 16:15:10,517 : INFO : PROGRESS: at example #710000, processed 15898729 words (4535520/s), 117642 word types, 710000 tags\n",
      "2019-01-23 16:15:10,567 : INFO : PROGRESS: at example #720000, processed 16124893 words (4533457/s), 118307 word types, 720000 tags\n",
      "2019-01-23 16:15:10,618 : INFO : PROGRESS: at example #730000, processed 16348485 words (4447716/s), 118951 word types, 730000 tags\n",
      "2019-01-23 16:15:10,667 : INFO : PROGRESS: at example #740000, processed 16572057 words (4533152/s), 119708 word types, 740000 tags\n",
      "2019-01-23 16:15:10,722 : INFO : PROGRESS: at example #750000, processed 16796401 words (4146520/s), 120414 word types, 750000 tags\n",
      "2019-01-23 16:15:10,772 : INFO : PROGRESS: at example #760000, processed 17018535 words (4492701/s), 121088 word types, 760000 tags\n",
      "2019-01-23 16:15:10,822 : INFO : PROGRESS: at example #770000, processed 17243121 words (4489042/s), 121744 word types, 770000 tags\n",
      "2019-01-23 16:15:10,870 : INFO : PROGRESS: at example #780000, processed 17456218 words (4508775/s), 122445 word types, 780000 tags\n",
      "2019-01-23 16:15:10,919 : INFO : PROGRESS: at example #790000, processed 17678724 words (4545789/s), 123132 word types, 790000 tags\n",
      "2019-01-23 16:15:10,947 : INFO : collected 123505 word types and 795538 unique tags from a corpus of 795538 examples and 17798270 words\n",
      "2019-01-23 16:15:10,947 : INFO : Loading a fresh vocabulary\n",
      "2019-01-23 16:15:11,038 : INFO : effective_min_count=2 retains 74452 unique words (60% of original 123505, drops 49053)\n",
      "2019-01-23 16:15:11,038 : INFO : effective_min_count=2 leaves 17749217 word corpus (99% of original 17798270, drops 49053)\n",
      "2019-01-23 16:15:11,178 : INFO : deleting the raw counts dictionary of 123505 items\n",
      "2019-01-23 16:15:11,181 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-01-23 16:15:11,181 : INFO : downsampling leaves estimated 13317951 word corpus (75.0% of prior 17749217)\n",
      "2019-01-23 16:15:11,361 : INFO : estimated required memory for 74452 words and 100 dimensions: 574110400 bytes\n",
      "2019-01-23 16:15:11,361 : INFO : resetting layer weights\n",
      "2019-01-23 16:15:17,474 : INFO : training model with 3 workers on 74452 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-01-23 16:15:18,491 : INFO : EPOCH 1 - PROGRESS: at 4.12% examples, 578224 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:19,505 : INFO : EPOCH 1 - PROGRESS: at 8.04% examples, 562224 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:20,506 : INFO : EPOCH 1 - PROGRESS: at 12.01% examples, 562023 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:21,510 : INFO : EPOCH 1 - PROGRESS: at 15.93% examples, 561472 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:22,512 : INFO : EPOCH 1 - PROGRESS: at 20.06% examples, 564472 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:23,524 : INFO : EPOCH 1 - PROGRESS: at 24.02% examples, 564159 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:24,529 : INFO : EPOCH 1 - PROGRESS: at 28.02% examples, 564656 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:25,548 : INFO : EPOCH 1 - PROGRESS: at 31.95% examples, 562092 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:26,548 : INFO : EPOCH 1 - PROGRESS: at 36.09% examples, 564642 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:27,553 : INFO : EPOCH 1 - PROGRESS: at 40.12% examples, 564984 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:28,559 : INFO : EPOCH 1 - PROGRESS: at 44.21% examples, 565885 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:29,559 : INFO : EPOCH 1 - PROGRESS: at 47.87% examples, 561616 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:30,581 : INFO : EPOCH 1 - PROGRESS: at 51.66% examples, 558814 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:31,590 : INFO : EPOCH 1 - PROGRESS: at 54.89% examples, 551418 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:32,613 : INFO : EPOCH 1 - PROGRESS: at 58.25% examples, 545507 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:33,632 : INFO : EPOCH 1 - PROGRESS: at 61.67% examples, 540515 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:34,636 : INFO : EPOCH 1 - PROGRESS: at 64.89% examples, 535631 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:35,642 : INFO : EPOCH 1 - PROGRESS: at 68.11% examples, 531227 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:36,656 : INFO : EPOCH 1 - PROGRESS: at 70.87% examples, 523380 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:37,673 : INFO : EPOCH 1 - PROGRESS: at 73.69% examples, 516612 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:38,678 : INFO : EPOCH 1 - PROGRESS: at 77.08% examples, 514526 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:39,687 : INFO : EPOCH 1 - PROGRESS: at 80.95% examples, 515459 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:40,711 : INFO : EPOCH 1 - PROGRESS: at 84.41% examples, 513503 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:41,713 : INFO : EPOCH 1 - PROGRESS: at 87.56% examples, 510552 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:42,715 : INFO : EPOCH 1 - PROGRESS: at 90.88% examples, 508794 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:43,737 : INFO : EPOCH 1 - PROGRESS: at 94.19% examples, 506756 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:44,756 : INFO : EPOCH 1 - PROGRESS: at 97.48% examples, 504681 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:45,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-23 16:15:45,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-23 16:15:45,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-23 16:15:45,535 : INFO : EPOCH - 1 : training on 17798270 raw words (14114464 effective words) took 28.1s, 503064 effective words/s\n",
      "2019-01-23 16:15:46,545 : INFO : EPOCH 2 - PROGRESS: at 3.40% examples, 481328 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:47,559 : INFO : EPOCH 2 - PROGRESS: at 7.02% examples, 494638 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:48,565 : INFO : EPOCH 2 - PROGRESS: at 11.00% examples, 515802 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:49,592 : INFO : EPOCH 2 - PROGRESS: at 14.82% examples, 519731 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:50,592 : INFO : EPOCH 2 - PROGRESS: at 18.02% examples, 506191 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:51,608 : INFO : EPOCH 2 - PROGRESS: at 21.31% examples, 498280 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:52,621 : INFO : EPOCH 2 - PROGRESS: at 24.84% examples, 498535 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:53,658 : INFO : EPOCH 2 - PROGRESS: at 27.58% examples, 482579 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:54,673 : INFO : EPOCH 2 - PROGRESS: at 30.39% examples, 472406 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:55,685 : INFO : EPOCH 2 - PROGRESS: at 32.77% examples, 458810 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:56,689 : INFO : EPOCH 2 - PROGRESS: at 35.75% examples, 455152 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:15:57,699 : INFO : EPOCH 2 - PROGRESS: at 38.53% examples, 449880 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:15:58,700 : INFO : EPOCH 2 - PROGRESS: at 41.63% examples, 448823 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-23 16:15:59,709 : INFO : EPOCH 2 - PROGRESS: at 45.05% examples, 450955 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:00,711 : INFO : EPOCH 2 - PROGRESS: at 48.14% examples, 449822 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:01,730 : INFO : EPOCH 2 - PROGRESS: at 51.95% examples, 454777 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:02,755 : INFO : EPOCH 2 - PROGRESS: at 55.24% examples, 454777 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:03,758 : INFO : EPOCH 2 - PROGRESS: at 58.42% examples, 454493 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:04,771 : INFO : EPOCH 2 - PROGRESS: at 61.44% examples, 452373 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:05,772 : INFO : EPOCH 2 - PROGRESS: at 65.16% examples, 456190 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:06,780 : INFO : EPOCH 2 - PROGRESS: at 68.78% examples, 458782 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:07,782 : INFO : EPOCH 2 - PROGRESS: at 72.73% examples, 463034 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:08,783 : INFO : EPOCH 2 - PROGRESS: at 76.53% examples, 465898 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:09,786 : INFO : EPOCH 2 - PROGRESS: at 80.38% examples, 468880 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:10,786 : INFO : EPOCH 2 - PROGRESS: at 84.14% examples, 471004 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:11,794 : INFO : EPOCH 2 - PROGRESS: at 87.96% examples, 473399 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:12,803 : INFO : EPOCH 2 - PROGRESS: at 91.77% examples, 475614 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:13,820 : INFO : EPOCH 2 - PROGRESS: at 95.64% examples, 477818 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:14,830 : INFO : EPOCH 2 - PROGRESS: at 99.53% examples, 479709 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:14,919 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-23 16:16:14,935 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-23 16:16:14,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-23 16:16:14,937 : INFO : EPOCH - 2 : training on 17798270 raw words (14113297 effective words) took 29.4s, 480118 effective words/s\n",
      "2019-01-23 16:16:15,947 : INFO : EPOCH 3 - PROGRESS: at 3.84% examples, 543366 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:16,962 : INFO : EPOCH 3 - PROGRESS: at 7.70% examples, 540546 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:17,979 : INFO : EPOCH 3 - PROGRESS: at 11.61% examples, 541982 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:18,983 : INFO : EPOCH 3 - PROGRESS: at 15.43% examples, 542264 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:19,987 : INFO : EPOCH 3 - PROGRESS: at 19.22% examples, 539596 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:20,993 : INFO : EPOCH 3 - PROGRESS: at 23.04% examples, 540028 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:22,002 : INFO : EPOCH 3 - PROGRESS: at 26.86% examples, 540232 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:23,006 : INFO : EPOCH 3 - PROGRESS: at 30.72% examples, 540752 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:24,016 : INFO : EPOCH 3 - PROGRESS: at 34.53% examples, 539902 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:25,017 : INFO : EPOCH 3 - PROGRESS: at 38.26% examples, 538892 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:26,020 : INFO : EPOCH 3 - PROGRESS: at 42.08% examples, 538723 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:27,020 : INFO : EPOCH 3 - PROGRESS: at 45.84% examples, 538024 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:28,025 : INFO : EPOCH 3 - PROGRESS: at 49.72% examples, 538433 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:29,033 : INFO : EPOCH 3 - PROGRESS: at 53.58% examples, 538629 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:30,051 : INFO : EPOCH 3 - PROGRESS: at 57.42% examples, 538461 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:31,063 : INFO : EPOCH 3 - PROGRESS: at 61.33% examples, 538560 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:32,097 : INFO : EPOCH 3 - PROGRESS: at 65.26% examples, 538872 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:33,098 : INFO : EPOCH 3 - PROGRESS: at 69.11% examples, 539255 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:34,109 : INFO : EPOCH 3 - PROGRESS: at 72.68% examples, 536873 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:35,137 : INFO : EPOCH 3 - PROGRESS: at 76.76% examples, 537753 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:36,172 : INFO : EPOCH 3 - PROGRESS: at 80.67% examples, 537307 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:37,192 : INFO : EPOCH 3 - PROGRESS: at 84.58% examples, 537224 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:38,214 : INFO : EPOCH 3 - PROGRESS: at 88.48% examples, 537088 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:39,224 : INFO : EPOCH 3 - PROGRESS: at 92.35% examples, 537214 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:40,227 : INFO : EPOCH 3 - PROGRESS: at 96.14% examples, 537167 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:41,186 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-23 16:16:41,195 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-23 16:16:41,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-23 16:16:41,200 : INFO : EPOCH - 3 : training on 17798270 raw words (14112907 effective words) took 26.3s, 537456 effective words/s\n",
      "2019-01-23 16:16:42,216 : INFO : EPOCH 4 - PROGRESS: at 3.90% examples, 547306 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:43,216 : INFO : EPOCH 4 - PROGRESS: at 7.24% examples, 511214 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:44,222 : INFO : EPOCH 4 - PROGRESS: at 10.02% examples, 471839 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:45,239 : INFO : EPOCH 4 - PROGRESS: at 13.27% examples, 466658 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:46,264 : INFO : EPOCH 4 - PROGRESS: at 16.90% examples, 473974 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:47,268 : INFO : EPOCH 4 - PROGRESS: at 19.71% examples, 460691 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:48,297 : INFO : EPOCH 4 - PROGRESS: at 23.71% examples, 474152 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:49,303 : INFO : EPOCH 4 - PROGRESS: at 27.13% examples, 475846 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:50,316 : INFO : EPOCH 4 - PROGRESS: at 30.72% examples, 478594 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:51,326 : INFO : EPOCH 4 - PROGRESS: at 34.24% examples, 480139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:52,335 : INFO : EPOCH 4 - PROGRESS: at 37.15% examples, 473601 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:53,372 : INFO : EPOCH 4 - PROGRESS: at 39.67% examples, 462553 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:54,384 : INFO : EPOCH 4 - PROGRESS: at 42.26% examples, 454735 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:55,388 : INFO : EPOCH 4 - PROGRESS: at 45.29% examples, 452682 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:56,412 : INFO : EPOCH 4 - PROGRESS: at 48.88% examples, 455508 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:57,412 : INFO : EPOCH 4 - PROGRESS: at 52.18% examples, 456196 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:16:58,423 : INFO : EPOCH 4 - PROGRESS: at 55.81% examples, 459249 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:16:59,443 : INFO : EPOCH 4 - PROGRESS: at 59.61% examples, 463072 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:00,461 : INFO : EPOCH 4 - PROGRESS: at 63.40% examples, 466148 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:01,473 : INFO : EPOCH 4 - PROGRESS: at 67.16% examples, 469408 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:02,475 : INFO : EPOCH 4 - PROGRESS: at 70.98% examples, 472649 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:03,475 : INFO : EPOCH 4 - PROGRESS: at 74.50% examples, 473466 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:04,488 : INFO : EPOCH 4 - PROGRESS: at 78.45% examples, 476684 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:05,514 : INFO : EPOCH 4 - PROGRESS: at 81.17% examples, 472213 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:06,539 : INFO : EPOCH 4 - PROGRESS: at 83.56% examples, 466242 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-23 16:17:07,635 : INFO : EPOCH 4 - PROGRESS: at 85.75% examples, 458576 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:08,667 : INFO : EPOCH 4 - PROGRESS: at 88.53% examples, 455481 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:09,678 : INFO : EPOCH 4 - PROGRESS: at 90.94% examples, 451264 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:10,720 : INFO : EPOCH 4 - PROGRESS: at 93.75% examples, 448734 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:11,731 : INFO : EPOCH 4 - PROGRESS: at 97.00% examples, 448892 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:12,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-23 16:17:12,737 : INFO : EPOCH 4 - PROGRESS: at 99.94% examples, 447379 words/s, in_qsize 1, out_qsize 1\n",
      "2019-01-23 16:17:12,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-23 16:17:12,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-23 16:17:12,757 : INFO : EPOCH - 4 : training on 17798270 raw words (14115142 effective words) took 31.6s, 447345 effective words/s\n",
      "2019-01-23 16:17:13,792 : INFO : EPOCH 5 - PROGRESS: at 2.39% examples, 331021 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:14,814 : INFO : EPOCH 5 - PROGRESS: at 4.45% examples, 308798 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:15,823 : INFO : EPOCH 5 - PROGRESS: at 7.70% examples, 357042 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:16,855 : INFO : EPOCH 5 - PROGRESS: at 11.12% examples, 385094 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:17,905 : INFO : EPOCH 5 - PROGRESS: at 14.61% examples, 403336 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:18,916 : INFO : EPOCH 5 - PROGRESS: at 17.24% examples, 397556 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:19,924 : INFO : EPOCH 5 - PROGRESS: at 19.76% examples, 391351 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:20,951 : INFO : EPOCH 5 - PROGRESS: at 22.16% examples, 383843 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:21,967 : INFO : EPOCH 5 - PROGRESS: at 24.62% examples, 380107 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:22,969 : INFO : EPOCH 5 - PROGRESS: at 27.14% examples, 377685 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:24,000 : INFO : EPOCH 5 - PROGRESS: at 29.89% examples, 377596 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:25,019 : INFO : EPOCH 5 - PROGRESS: at 32.28% examples, 373990 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:26,048 : INFO : EPOCH 5 - PROGRESS: at 34.99% examples, 373639 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:27,063 : INFO : EPOCH 5 - PROGRESS: at 37.37% examples, 370919 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:28,096 : INFO : EPOCH 5 - PROGRESS: at 38.64% examples, 357806 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:29,111 : INFO : EPOCH 5 - PROGRESS: at 40.40% examples, 350616 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:30,124 : INFO : EPOCH 5 - PROGRESS: at 42.44% examples, 346587 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:31,150 : INFO : EPOCH 5 - PROGRESS: at 44.44% examples, 342742 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:32,167 : INFO : EPOCH 5 - PROGRESS: at 46.64% examples, 340685 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:33,182 : INFO : EPOCH 5 - PROGRESS: at 48.77% examples, 338477 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:34,200 : INFO : EPOCH 5 - PROGRESS: at 51.11% examples, 337890 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:35,201 : INFO : EPOCH 5 - PROGRESS: at 53.30% examples, 336557 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:36,211 : INFO : EPOCH 5 - PROGRESS: at 55.70% examples, 336579 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:37,246 : INFO : EPOCH 5 - PROGRESS: at 58.25% examples, 337213 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:38,284 : INFO : EPOCH 5 - PROGRESS: at 60.81% examples, 337464 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:39,311 : INFO : EPOCH 5 - PROGRESS: at 63.50% examples, 338730 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:40,330 : INFO : EPOCH 5 - PROGRESS: at 66.66% examples, 342573 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:41,331 : INFO : EPOCH 5 - PROGRESS: at 69.58% examples, 344988 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:42,356 : INFO : EPOCH 5 - PROGRESS: at 71.98% examples, 344543 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:43,368 : INFO : EPOCH 5 - PROGRESS: at 74.38% examples, 344009 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:44,383 : INFO : EPOCH 5 - PROGRESS: at 77.08% examples, 344972 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:45,387 : INFO : EPOCH 5 - PROGRESS: at 79.87% examples, 346260 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:46,415 : INFO : EPOCH 5 - PROGRESS: at 82.12% examples, 345107 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:47,419 : INFO : EPOCH 5 - PROGRESS: at 84.68% examples, 345397 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:48,441 : INFO : EPOCH 5 - PROGRESS: at 87.12% examples, 345034 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:49,466 : INFO : EPOCH 5 - PROGRESS: at 89.59% examples, 344879 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-23 16:17:50,470 : INFO : EPOCH 5 - PROGRESS: at 92.18% examples, 345343 words/s, in_qsize 5, out_qsize 0\n",
      "2019-01-23 16:17:51,498 : INFO : EPOCH 5 - PROGRESS: at 94.81% examples, 345780 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:52,509 : INFO : EPOCH 5 - PROGRESS: at 97.42% examples, 346145 words/s, in_qsize 6, out_qsize 0\n",
      "2019-01-23 16:17:53,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-23 16:17:53,429 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-23 16:17:53,436 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-23 16:17:53,436 : INFO : EPOCH - 5 : training on 17798270 raw words (14113590 effective words) took 40.7s, 347015 effective words/s\n",
      "2019-01-23 16:17:53,437 : INFO : training on a 88991350 raw words (70569400 effective words) took 156.0s, 452479 effective words/s\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model = gensim.models.Doc2Vec(tagged_data, \n",
    "                                        workers=3, vector_size=100, min_count=2, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends'], tags=['0'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['this', 'is', 'just', 'how', 'the', 'media', 'works', 'it', 'is', 'human', 'nature', 'to', 'reflect', 'biases', 'of', 'the', 'time'], tags=['488099'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[488099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-23 16:24:32,438 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('488099', 0.6928351521492004),\n",
       " ('99529', 0.688252329826355),\n",
       " ('572724', 0.6844782829284668),\n",
       " ('495022', 0.6844678521156311),\n",
       " ('566963', 0.6822594404220581),\n",
       " ('329759', 0.6795724630355835),\n",
       " ('209664', 0.6776979565620422),\n",
       " ('608301', 0.6775181293487549),\n",
       " ('34499', 0.6767327785491943),\n",
       " ('534405', 0.6750091314315796)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.docvecs.most_similar(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00657451, -0.00673204,  0.01914348,  0.00261382, -0.02546103,\n",
       "        0.00035164,  0.00837968, -0.0116435 ,  0.01599422,  0.0022529 ,\n",
       "        0.00593499, -0.03178662, -0.0406614 , -0.03296725, -0.00272632,\n",
       "        0.07900023,  0.03294246,  0.03339288,  0.03553646, -0.01249552,\n",
       "       -0.00848655,  0.01112831,  0.00839194,  0.00625079, -0.00747775,\n",
       "        0.02255589, -0.03087612,  0.00848052,  0.0241745 ,  0.0170817 ,\n",
       "        0.0355262 , -0.01813187, -0.01314204,  0.02231872, -0.06392603,\n",
       "        0.03032535,  0.01256634,  0.0078778 ,  0.02457006,  0.01014347,\n",
       "       -0.03184626, -0.00026925,  0.00484611,  0.02548797, -0.01674787,\n",
       "       -0.01106067,  0.02835838, -0.01333592,  0.02243997,  0.00728079,\n",
       "        0.00763755,  0.00590347,  0.00588352, -0.0169645 , -0.03153446,\n",
       "        0.01947374, -0.00980859,  0.00339037, -0.02047027, -0.03282501,\n",
       "        0.0227222 , -0.02108854,  0.05636634,  0.01098596, -0.00639193,\n",
       "        0.00982421,  0.02245803, -0.01547608, -0.00955821,  0.02022522,\n",
       "       -0.0386003 ,  0.03835051,  0.02842727, -0.02042913, -0.00394667,\n",
       "       -0.00296384, -0.03472346, -0.04432174,  0.00479399,  0.00588841,\n",
       "        0.00165465, -0.00761571, -0.01297032, -0.03215456, -0.0138649 ,\n",
       "       -0.04226957, -0.00440727,  0.01629661, -0.00266632, -0.02215544,\n",
       "       -0.04111299, -0.02454022, -0.00483973,  0.0259128 , -0.00017968,\n",
       "       -0.00213696,  0.02480292, -0.01145736,  0.01230029, -0.02232688],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.docvecs[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-23 16:18:31,523 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prince', 0.5985826253890991),\n",
       " ('queen', 0.5685798525810242),\n",
       " ('princess', 0.5472434759140015),\n",
       " ('countess', 0.5304868221282959),\n",
       " ('bride', 0.527978777885437),\n",
       " ('commoner', 0.5078327655792236),\n",
       " ('belle', 0.5065819025039673),\n",
       " ('cinderella', 0.49380117654800415),\n",
       " ('dracula', 0.4893835783004761),\n",
       " ('duchess', 0.475595623254776)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.wv.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "train_data, test_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_data['review_cleaned'] = train_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "test_data['review_cleaned'] = test_data.review.apply(lambda x: utils.normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, model):\n",
    "    return model.infer_vector(text.split(\" \"))\n",
    "\n",
    "train_vectors = [get_vector(x, doc2vec_model) for x in train_data['review_cleaned']]\n",
    "train_vectors=np.array(train_vectors)\n",
    "\n",
    "test_vectors = [get_vector(x, doc2vec_model) for x in test_data['review_cleaned']]\n",
    "test_vectors=np.array(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_vectors\n",
    "y_train = train_data.sentiment\n",
    "\n",
    "x_test = test_vectors\n",
    "y_test  = test_data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/andryw/Projects/tutorials/python_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8317936007210456"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(verbose=True, n_jobs=3)\n",
    "logistic_regression = logistic_regression.fit(x_train, y_train)\n",
    "metrics.roc_auc_score(logistic_regression.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\"n_jobs\": 3, \"silent\": True, \"n_estimators\": 100, 'objective':'binary:logistic'}\n",
    "# xgboost_classifier = xgb.XGBClassifier(**params)\n",
    "\n",
    "# xgboost_classifier.fit(x_train, y_train, verbose=True, eval_metric='auc',\n",
    "#                       eval_set=[(x_train, y_train), (x_test, y_test)],)\n",
    "\n",
    "# metrics.roc_auc_score(xgboost_classifier.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabel_data['review_cleaned'] = unlabel_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "unlabel_vectors = [get_vector(x, doc2vec_model) for x in unlabel_data['review_cleaned']]\n",
    "unlabel_vectors=np.array(unlabel_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_predict(unlabel_data, \n",
    "                   logistic_regression.predict(unlabel_vectors), \n",
    "                   \"logistic_doc2vec_window-5_without-stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
