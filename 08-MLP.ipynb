{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim \n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import keras\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "train_data, test_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "train_data['review_cleaned'] = train_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "test_data['review_cleaned'] = test_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "\n",
    "unlabel_data = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabel_data['review_cleaned'] = unlabel_data.review.apply(lambda x: utils.normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_words(text_list):\n",
    "    text_set = set()\n",
    "    for text in text_list:\n",
    "        text_set |= set(text.split())\n",
    "    return len(text_set)\n",
    "\n",
    "def get_max_size_of_sentence(text_list):\n",
    "    max_size = 0\n",
    "    for text in text_list:\n",
    "        max_size = max(max_size, len(text))\n",
    "    return max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the total of unique words\n",
    "total_words = get_total_words(list(train_data['review_cleaned']) + \n",
    "                list(test_data['review_cleaned']) +\n",
    "                list(unlabel_data['review_cleaned']))\n",
    "\n",
    "## Find max size of a setence\n",
    "max_size = get_max_size_of_sentence(list(train_data['review_cleaned']) + \n",
    "                list(test_data['review_cleaned']) +\n",
    "                list(unlabel_data['review_cleaned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words=50000\n",
    "tokenizer = preprocessing.text.Tokenizer(total_words)\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_data['review_cleaned']) + \n",
    "                       list(test_data['review_cleaned']) +\n",
    "                       list(unlabel_data['review_cleaned'])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=total_words):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "train_one_hot_index = vectorize_sequences(tokenizer.texts_to_sequences(list(train_data['review_cleaned'])))\n",
    "test_one_hot_index = vectorize_sequences(tokenizer.texts_to_sequences(list(test_data['review_cleaned'])))\n",
    "unlabel_data_one_hot_index = vectorize_sequences(tokenizer.texts_to_sequences(list(unlabel_data['review_cleaned'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_one_hot_index\n",
    "y_train = train_data.sentiment\n",
    "\n",
    "x_test = test_one_hot_index\n",
    "y_test  = test_data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                1600032   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,600,849\n",
      "Trainable params: 1,600,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation=\"relu\", input_shape=(total_words,)))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/1\n",
      "18750/18750 [==============================] - 11s 603us/step - loss: 0.3410 - acc: 0.8641 - val_loss: 0.2952 - val_acc: 0.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59b8a03860>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [x[0] for x in model.predict_classes(unlabel_data_one_hot_index)]\n",
    "utils.save_predict(unlabel_data, \n",
    "                   prediction, \n",
    "                   \"mlp_50k-words-3-layers_rmsprop_1-epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
