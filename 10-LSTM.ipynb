{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim \n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import keras\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, SimpleRNN, LSTM, CuDNNLSTM, Dropout\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "train_data, test_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "train_data['review_cleaned'] = train_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "test_data['review_cleaned'] = test_data.review.apply(lambda x: utils.normalize_text(x))\n",
    "\n",
    "unlabel_data = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabel_data['review_cleaned'] = unlabel_data.review.apply(lambda x: utils.normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def get_total_words(text_list):\n",
    "    text_set = set()\n",
    "    for text in text_list:\n",
    "        text_set |= set(text.split())\n",
    "    return len(text_set)\n",
    "\n",
    "def get_max_size_of_sentence(text_list):\n",
    "    max_size = 0\n",
    "    for text in text_list:\n",
    "        max_size = max(max_size, len(text))\n",
    "    return max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the total of unique words\n",
    "total_words = get_total_words(list(train_data['review_cleaned']) + \n",
    "                list(test_data['review_cleaned']) +\n",
    "                list(unlabel_data['review_cleaned']))\n",
    "\n",
    "## Find max size of a setence\n",
    "max_size = get_max_size_of_sentence(list(train_data['review_cleaned']) + \n",
    "                list(test_data['review_cleaned']) +\n",
    "                list(unlabel_data['review_cleaned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer(total_words)\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_data['review_cleaned']) + \n",
    "                       list(test_data['review_cleaned']) +\n",
    "                       list(unlabel_data['review_cleaned'])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_index = tokenizer.texts_to_sequences(list(train_data['review_cleaned']))\n",
    "train_one_hot_index = preprocessing.sequence.pad_sequences(train_one_hot_index, max_size)\n",
    "\n",
    "test_one_hot_index = tokenizer.texts_to_sequences(list(test_data['review_cleaned']))\n",
    "test_one_hot_index = preprocessing.sequence.pad_sequences(test_one_hot_index, max_size)\n",
    "\n",
    "unlabel_data_one_hot_index = tokenizer.texts_to_sequences(list(unlabel_data['review_cleaned']))\n",
    "unlabel_data_one_hot_index = preprocessing.sequence.pad_sequences(unlabel_data_one_hot_index, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_one_hot_index\n",
    "y_train = train_data.sentiment\n",
    "\n",
    "x_test = test_one_hot_index\n",
    "y_test  = test_data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_size))\n",
    "    model.add(SimpleRNN(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 9434, 100)         10124600  \n",
      "_________________________________________________________________\n",
      "simple_rnn_11 (SimpleRNN)    (None, 32)                4256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 10,128,889\n",
      "Trainable params: 10,128,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_rnn = create_simple_rnn()\n",
    "simple_rnn.summary()\n",
    "simple_rnn.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/1\n",
      "18750/18750 [==============================] - 941s 50ms/step - loss: 0.4545 - acc: 0.7806 - val_loss: 0.3389 - val_acc: 0.8552\n"
     ]
    }
   ],
   "source": [
    "history = simple_rnn.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 9434, 100)         10124600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 10,142,169\n",
      "Trainable params: 10,142,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_lstm(dense_layers=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_size))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    if dense_layers:\n",
    "        for i in range(dense_layers):\n",
    "            model.add(Dense(16, activation=\"relu\"))\n",
    "            model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "lstm = create_lstm(dense_layers=1)\n",
    "lstm.summary()\n",
    "lstm.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/1\n",
      "18750/18750 [==============================] - 2573s 137ms/step - loss: 0.3928 - acc: 0.8332 - val_loss: 0.2684 - val_acc: 0.8909\n"
     ]
    }
   ],
   "source": [
    "history = lstm.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [x[0] for x in simple_rnn.predict_classes(unlabel_data_one_hot_index)]\n",
    "utils.save_predict(unlabel_data, \n",
    "                   prediction, \n",
    "                   \"rnn_1-epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [x[0] for x in lstm.predict_classes(unlabel_data_one_hot_index)]\n",
    "utils.save_predict(unlabel_data, \n",
    "                   prediction, \n",
    "                   \"lstm_dense-1_1-epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
